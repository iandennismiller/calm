#!/usr/bin/env python3

import os
import sys
import json

import click
from dotenv import load_dotenv

from calm import Calm
from calm.utils import get_resource_max
from calm.llm import LLM
from calm.context import context_to_string


@click.group()
def cli():
    pass

@cli.command()
@click.option('-m', '--model', type=str, required=False, default="mistral-openorca")
@click.option('-c', '--character', type=str, required=False)
@click.option('-q', '--quiet', is_flag=True, default=False)
@click.option('-p', '--path', type=str, required=False, default="")
@click.option('-h', '--host', type=str, required=False, default="")
@click.option('-n', '--num', type=int, required=False, default=10)
@click.option('-l', '--localhost', is_flag=True, default=False)
@click.option('-k', '--kb', is_flag=True, default=False)
@click.argument('words', type=str)
def say(words, model, character, quiet, path, host, num, localhost, kb):
    "Say some words to a model or character"
    # validate input
    if localhost is True:
        host = "localhost:7999"
    if kb is True and path == "":
        path = os.path.expanduser("~/.local/share/calm")

    if host == "":
        host = None
        port = None
    else:
        host_split = host.split(":")
        if len(host_split) == 1:
            port = 7999
        else:
            host = host_split[0]
            port = int(host_split[1])

    if path == "":
        path = None

    calm = Calm()

    if path is not None or host is not None:
        calm.get_kb(chroma_persist_dir=path, host=host, port=port)

    instance = calm.get_instance(model, character, quiet)
    answer = calm.answer(instance, words)
    print(answer)

@cli.command()
@click.option('-m', '--model', type=str, required=False)
@click.option('-c', '--character', type=str, required=False)
@click.option('-q', '--quiet', is_flag=True, default=False)
def api(model, character, quiet):
    "Run the API (compatible w/ OpenAI)"
    calm = Calm()
    instance = calm.get_instance(model, character, quiet)
    calm.api(instance)

@cli.command('list')
@click.option('--health', is_flag=True, default=False)
@click.option('--nsfw', is_flag=True, default=False)
def list_models(health, nsfw):
    "List available model releases"
    calm = Calm()
    for model_name in calm.list_models():
        instance = LLM.from_config(name=model_name)
        if instance.qualification not in ["health", "nsfw"]:
            print(model_name)

    if health is True or nsfw is True:
        print("--- The following models are not for general use ---")
        for model_name in calm.list_models():
            instance = LLM.from_config(name=model_name)
            if instance.qualification == "health" and health is True:
                print(f"HEALTH: {model_name}")
            elif instance.qualification == "nsfw" and nsfw is True:
                print(f"NFSW:    {model_name}")

@cli.command()
@click.argument('model_name', type=str, default="mistral-openorca")
@click.option('--health', is_flag=True, default=False)
@click.option('--nsfw', is_flag=True, default=False)
def download(model_name, health, nsfw):
    "Download a model"
    calm = Calm()
    if model_name in calm.list_models():
        instance = LLM.from_config(name=model_name)
        if instance.qualification not in ["health", "nsfw"] or (instance.qualification == "health" and health is True) or (instance.qualification == "nsfw" and nsfw is True):
            print(f"Downloading {instance}...")
            instance.download()
            print(f"Downloaded model to {instance.resolve_path()}")
        else:
            print(f"That model is not for general use. Try `calm list` to see available models.")
    else:
        print("That model is not available. Try `calm list` to see available models.")

@cli.command('max')
def list_max():
    "Max model sizes supported by your system"
    for size in ["180b", "70b", "30b", "13b", "7b", "3b", "1b"]:
        resource_max = get_resource_max(size)
        if resource_max:
            print(f"{size}\t{resource_max['quant']} quant\t{resource_max['input_size']} context")
        else:
            print(f"{size}\ttoo big")

@cli.command()
def chat():
    "Chat with an LLM"
    print("Not yet implemented")

@cli.command()
def update():
    "Pull updated model data"
    print("Not yet implemented")

@cli.command()
@click.option('-p', '--path', type=str, required=False, default="")
@click.option('-h', '--host', type=str, required=False, default="")
@click.option('-m', '--metadata', type=str, required=False, default="")
@click.option('-l', '--localhost', is_flag=True, default=False)
@click.argument('content', type=str, required=True)
def learn(path, host, content, metadata, localhost):
    "Import content to a knowledgebase"

    # validate input
    if localhost is True:
        host = "localhost:7999"

    if host == "":
        host = None
        port = None
    else:
        host_split = host.split(":")
        if len(host_split) == 1:
            port = 7999
        else:
            host = host_split[0]
            port = int(host_split[1])

    if path == "":
        path = None

    if len(metadata) > 0:
        metadata = json.loads(metadata)
    else:
        metadata = {}

    calm = Calm()
    calm.get_kb(chroma_persist_dir=path, host=host, port=port)
    result = calm.add_knowledge(content=content, metadata=metadata)
    print(f"{result}: ok")

@cli.command()
@click.option('-p', '--path', type=str, required=False, default="")
@click.option('-h', '--host', type=str, required=False, default="")
@click.option('-n', '--num', type=int, required=False, default=10)
@click.option('-l', '--localhost', is_flag=True, default=False)
@click.argument('query', type=str, required=True)
def recall(path, host, query, num, localhost):
    "Query content from a knowledgebase"
    calm = Calm()

    # validate input
    if localhost is True:
        host = "localhost:7999"

    if host == "":
        host = None
        port = None
    else:
        host_split = host.split(":")
        if len(host_split) == 1:
            port = 7999
        else:
            host = host_split[0]
            port = int(host_split[1])

    if path == "":
        path = None

    calm.get_kb(chroma_persist_dir=path, host=host, port=port)
    results = calm.query_knowledge(query=query, top_num=num)
    print(context_to_string(results))


if __name__ == "__main__":
    load_dotenv()
    if len(sys.argv) == 1:
        cli.main(['--help'])
    else:
        cli()
