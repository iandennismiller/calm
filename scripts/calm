#!/usr/bin/env python3

import sys

import click
from dotenv import load_dotenv

from calm import Calm
from calm.utils import get_resource_max
from calm.llm import LLM


@click.group()
def cli():
    pass

@cli.command()
@click.option('-m', '--model', type=str, required=False)
@click.option('-c', '--character', type=str, required=False)
@click.option('-q', '--quiet', is_flag=True, default=False)
@click.argument('words', type=str)
def say(words, model, character, quiet):
    "Say some words to a model or character"
    calm = Calm()
    instance = calm.get_instance(model, character, quiet)
    answer = calm.answer(instance, words)
    print(answer)

@cli.command()
@click.option('-m', '--model', type=str, required=False)
@click.option('-c', '--character', type=str, required=False)
@click.option('-q', '--quiet', is_flag=True, default=False)
def api(model, character, quiet):
    "Run the API (compatible w/ OpenAI)"
    calm = Calm()
    instance = calm.get_instance(model, character, quiet)
    calm.api(instance)

@cli.command('list')
@click.option('--health', is_flag=True, default=False)
@click.option('--nsfw', is_flag=True, default=False)
def list_models(health, nsfw):
    "List available model releases"
    calm = Calm()
    for model_name in calm.list_models():
        instance = LLM.from_config(name=model_name)
        if instance.qualification not in ["health", "nsfw"]:
            print(model_name)

    if health is True or nsfw is True:
        print("--- The following models are not for general use ---")
        for model_name in calm.list_models():
            instance = LLM.from_config(name=model_name)
            if instance.qualification == "health" and health is True:
                print(f"HEALTH: {model_name}")
            elif instance.qualification == "nsfw" and nsfw is True:
                print(f"NFSW:    {model_name}")

@cli.command()
@click.argument('model_name', type=str, default="mistral")
@click.option('--health', is_flag=True, default=False)
@click.option('--nsfw', is_flag=True, default=False)
def download(model_name, health, nsfw):
    "Download a model"
    calm = Calm()
    if model_name in calm.list_models():
        instance = LLM.from_config(name=model_name)
        if instance.qualification not in ["health", "nsfw"] or (instance.qualification == "health" and health is True) or (instance.qualification == "nsfw" and nsfw is True):
            print(f"Downloading {instance}...")
            instance.download()
            print(f"Downloaded model to {instance.resolve_path()}")
        else:
            print(f"That model is not for general use. Try `calm list` to see available models.")
    else:
        print("That model is not available. Try `calm list` to see available models.")

@cli.command('max')
def list_max():
    "Max model sizes supported by your system"
    for size in ["180b", "70b", "30b", "13b", "7b", "3b", "1b"]:
        resource_max = get_resource_max(size)
        if resource_max:
            print(f"{size}\t{resource_max['quant']} quant\t{resource_max['input_size']} context")
        else:
            print(f"{size}\ttoo big")

@cli.command()
def chat():
    "Chat with an LLM"
    print("Not yet implemented")

@cli.command()
def update():
    "Pull updated model data"
    print("Not yet implemented")


if __name__ == "__main__":
    load_dotenv()
    if len(sys.argv) == 1:
        cli.main(['--help'])
    else:
        cli()
