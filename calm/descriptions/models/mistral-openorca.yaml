name: Mistral OpenOrca
creator: OpenOrca
about: |
  We have used our own OpenOrca dataset to fine-tune on top of Mistral 7B. This dataset is our attempt to reproduce the dataset generated for Microsoft Research's Orca Paper. We use OpenChat packing, trained with Axolotl.
input_size: 32768
source:
  7b:
    q6_k: https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GGUF/resolve/main/mistral-7b-openorca.Q6_K.gguf
    q4_k_s: https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GGUF/resolve/main/mistral-7b-openorca.Q4_K_S.gguf
template:
  default: |
    <|im_start|>system
    {system_prompt}<|im_end|>
    <|im_start|>input
    {input}<|im_end|>
    <|im_start|>user
    {prompt}<|im_end|>
    <|im_start|>assistant
  no_input: |
    <|im_start|>system
    {system_prompt}<|im_end|>
    <|im_start|>user
    {prompt}<|im_end|>
    <|im_start|>assistant
  stop: "<|im_start|>user"
character: chatter-gpt
qualification: false
